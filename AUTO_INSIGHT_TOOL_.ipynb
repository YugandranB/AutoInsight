{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpJYwBJCIW7B",
        "outputId": "fcbad772-56af-49b1-d7aa-e36d477229f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.14.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.155.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.0 (from gradio)\n",
            "  Downloading gradio_client-1.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.27.1)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.9.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (14.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.66.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading youtube_transcript_api-0.6.3-py3-none-any.whl (622 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.14.0-py3-none-any.whl (57.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.0-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.9/321.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=056f1b1184739200a807a6736696fba825836301893b3c6b87fb01e15a6b755a\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built fpdf\n",
            "Installing collected packages: pydub, fpdf, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, youtube-transcript-api, vaderSentiment, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 fastapi-0.115.8 ffmpy-0.5.0 fpdf-1.7.2 gradio-5.14.0 gradio-client-1.7.0 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.4 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.45.3 tomlkit-0.13.2 uvicorn-0.34.0 vaderSentiment-3.3.2 youtube-transcript-api-0.6.3\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy vaderSentiment youtube-transcript-api gradio pandas fpdf openpyxl google-api-python-client wordcloud matplotlib\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, VideoUnavailable\n",
        "from googleapiclient.discovery import build\n",
        "from fpdf import FPDF\n",
        "import pandas as pd\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.parse\n",
        "import gradio as gr\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spacy and VADER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# YouTube Data API key\n",
        "YOUTUBE_API_KEY = \"XXXXXXXXXXXXXXXXXXXXXXXXXX\"\n"
      ],
      "metadata": {
        "id": "MhXtERRLIdw9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_video_id(url):\n",
        "    try:\n",
        "        parsed_url = urllib.parse.urlparse(url)\n",
        "        if parsed_url.netloc in ['www.youtube.com', 'youtube.com']:\n",
        "            if parsed_url.query:\n",
        "                return urllib.parse.parse_qs(parsed_url.query)['v'][0]\n",
        "            if parsed_url.path:\n",
        "                match = re.search(r'/([a-zA-Z0-9_-]+)$', parsed_url.path)\n",
        "                if match:\n",
        "                  return match.group(1)\n",
        "        elif parsed_url.netloc in ['youtu.be']:\n",
        "            return parsed_url.path[1:]\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing URL: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "nHVHpgyYIdzS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_video_metadata(video_url):\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "      return None, \"Invalid Youtube URL format.\"\n",
        "\n",
        "    youtube = build(\"youtube\", \"v3\", developerKey=YOUTUBE_API_KEY)\n",
        "\n",
        "    try:\n",
        "        request = youtube.videos().list(part=\"snippet,statistics\", id=video_id)\n",
        "        response = request.execute()\n",
        "\n",
        "        if not response['items']:\n",
        "            return None, \"Video not found with provided url.\"\n",
        "\n",
        "        video_data = response['items'][0]\n",
        "\n",
        "        metadata = {\n",
        "            \"channel_name\": video_data['snippet']['channelTitle'],\n",
        "            \"video_title\": video_data['snippet']['title'],\n",
        "            \"views\": video_data['statistics']['viewCount'],\n",
        "            \"likes\": video_data['statistics'].get('likeCount', 'N/A'),\n",
        "            \"dislikes\": video_data['statistics'].get('dislikeCount', 'N/A'),\n",
        "            \"posted_date\": video_data['snippet']['publishedAt']\n",
        "        }\n",
        "\n",
        "        return metadata, None\n",
        "\n",
        "    except VideoUnavailable:\n",
        "        return None, \"Video is unavailable.\"\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "\n",
        "\n",
        "def fetch_transcript(video_url):\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        return None, \"Invalid Youtube URL format\"\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        text = \" \".join([t['text'] for t in transcript])\n",
        "        return text, None\n",
        "\n",
        "    except (TranscriptsDisabled, VideoUnavailable):\n",
        "        return None, \"Transcript not available for this video.\"\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "\n",
        "\n",
        "def split_long_sentences(text):\n",
        "    doc = nlp(text)  # Tokenize into sentences using Spacy\n",
        "    sentences = []\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        if len(sent.text.split()) > 25:\n",
        "            sub_sentences = []\n",
        "            current_chunk = []\n",
        "            for token in sent:\n",
        "                current_chunk.append(token.text)\n",
        "                if token.is_punct and token.text in {\".\", \"!\", \"?\"}:\n",
        "                    sub_sentences.append(\" \".join(current_chunk).strip())\n",
        "                    current_chunk = []\n",
        "                elif token.text.lower() in {\"and\", \"but\", \"because\", \"so\"}:\n",
        "                    if len(current_chunk) > 3:\n",
        "                        sub_sentences.append(\" \".join(current_chunk).strip())\n",
        "                        current_chunk = []\n",
        "\n",
        "            if current_chunk:\n",
        "                sub_sentences.append(\" \".join(current_chunk).strip())\n",
        "\n",
        "            sentences.extend(sub_sentences)\n",
        "        else:\n",
        "            sentences.append(sent.text.strip())\n",
        "\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "hXArcntaId3i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_keywords(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    attributes = df.columns.tolist()\n",
        "    keywords = {}\n",
        "\n",
        "    for attribute in attributes:\n",
        "        keywords[attribute] = df[attribute].dropna().tolist()\n",
        "\n",
        "    return keywords, attributes\n",
        "\n",
        "\n",
        "\n",
        "def match_keywords_in_sentences(sentences, keywords):\n",
        "    matched_keywords = {attribute: [] for attribute in keywords}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for attribute, sub_keywords in keywords.items():\n",
        "            for keyword in sub_keywords:\n",
        "                if keyword.lower() in sentence.lower():\n",
        "                    matched_keywords[attribute].append(sentence)\n",
        "\n",
        "    return matched_keywords\n"
      ],
      "metadata": {
        "id": "3ycGkk1bId56"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment_for_keywords(matched_keywords, sentences):\n",
        "    sentiment_results = {}\n",
        "\n",
        "    for attribute, sentences_list in matched_keywords.items():\n",
        "        positive_lines = []\n",
        "        negative_lines = []\n",
        "\n",
        "        for line in sentences_list:\n",
        "            sentiment = sia.polarity_scores(line)\n",
        "            if sentiment['compound'] > 0.05:\n",
        "                positive_lines.append((line.strip(), sentiment['compound']))\n",
        "            elif sentiment['compound'] < -0.05:\n",
        "                negative_lines.append((line.strip(), sentiment['compound']))\n",
        "\n",
        "        sentiment_results[attribute] = {\n",
        "            \"positive\": positive_lines,\n",
        "            \"negative\": negative_lines\n",
        "        }\n",
        "\n",
        "    return sentiment_results\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hb3ErUlpId8i"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.styles import Font, Alignment\n",
        "\n",
        "def generate_excel(sentiment_results, attributes):\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "    ws.title = \"Sentiment Analysis Results\"\n",
        "\n",
        "    # Set header font style\n",
        "    header_font = Font(bold=True)\n",
        "    alignment = Alignment(horizontal='center')\n",
        "\n",
        "    # Write headers\n",
        "    headers = [\"Attribute\", \"Sentiment\", \"Sentence\", \"Sentiment Score\"]\n",
        "    for col_num, header in enumerate(headers, 1):\n",
        "        cell = ws.cell(row=1, column=col_num)\n",
        "        cell.value = header\n",
        "        cell.font = header_font\n",
        "        cell.alignment = alignment\n",
        "\n",
        "    row_num = 2\n",
        "    for attribute, sentiment_data in sentiment_results.items():\n",
        "        for sentiment_type, sentences in sentiment_data.items():\n",
        "            for sentence, score in sentences:\n",
        "                ws.cell(row=row_num, column=1).value = attribute\n",
        "                ws.cell(row=row_num, column=2).value = sentiment_type.capitalize()\n",
        "                ws.cell(row=row_num, column=3).value = sentence\n",
        "                ws.cell(row=row_num, column=4).value = score\n",
        "                row_num += 1\n",
        "\n",
        "    # Auto-adjust column width\n",
        "    for col in ws.columns:\n",
        "        max_length = 0\n",
        "        column = col[0].column_letter # Get the column name\n",
        "        for cell in col:\n",
        "            try: # Necessary to avoid error on empty cells\n",
        "                if len(str(cell.value)) > max_length:\n",
        "                    max_length = len(cell.value)\n",
        "            except:\n",
        "                pass\n",
        "        adjusted_width = (max_length + 2) * 1.2\n",
        "        ws.column_dimensions[column].width = adjusted_width\n",
        "\n",
        "    # Save the Excel file\n",
        "    wb.save(\"sentiment_analysis_results.xlsx\")\n",
        "\n",
        "\n",
        "\n",
        "def generate_word_clouds(matched_keywords):\n",
        "    wordclouds = {}\n",
        "\n",
        "    for attribute, sentences_list in matched_keywords.items():\n",
        "        text = \" \".join(sentences_list)\n",
        "\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
        "        wordclouds[attribute] = wordcloud\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"Word Cloud for {attribute}\")\n",
        "        plt.show()\n",
        "\n",
        "    return wordclouds\n"
      ],
      "metadata": {
        "id": "o1z7TFVeId_C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pdf_with_sections(metadata, sentiment_results, wordclouds, output_file=\"Analysis_Report.pdf\"):\n",
        "    pdf = FPDF()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "\n",
        "    # Add Metadata\n",
        "    pdf.set_font(\"Arial\", \"B\", 16)\n",
        "    pdf.cell(200, 10, \"Auto-Insight: YouTube Video Sentiment & Attribute Analysis Report\", ln=True, align=\"C\")\n",
        "    pdf.ln(10)\n",
        "\n",
        "    if metadata:\n",
        "        pdf.set_font(\"Arial\", \"B\", 14)\n",
        "        pdf.cell(0, 10, \"Video Metadata\", ln=True)\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "        for key, value in metadata.items():\n",
        "            pdf.cell(0, 10, f\"{key.replace('_', ' ').title()}: {value}\", ln=True)\n",
        "        pdf.ln(10)\n",
        "\n",
        "    # Add Sections for Each Attribute\n",
        "    for attribute, sentiments in sentiment_results.items():\n",
        "        pdf.add_page()\n",
        "        pdf.set_font(\"Arial\", \"B\", 14)\n",
        "        pdf.cell(0, 10, f\"Attribute: {attribute}\", ln=True)\n",
        "        pdf.ln(5)\n",
        "\n",
        "        # Add Positive Sentiments\n",
        "        pdf.set_font(\"Arial\", \"B\", 12)\n",
        "        pdf.cell(0, 10, \"Positive Sentiments:\", ln=True)\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "        for line, score in sentiments[\"positive\"]:\n",
        "            pdf.multi_cell(0, 10, f\"Line: {line}\\nScore: {score}\")\n",
        "            pdf.ln(2)\n",
        "\n",
        "        # Add Negative Sentiments\n",
        "        pdf.set_font(\"Arial\", \"B\", 12)\n",
        "        pdf.cell(0, 10, \"Negative Sentiments:\", ln=True)\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "        for line, score in sentiments[\"negative\"]:\n",
        "            pdf.multi_cell(0, 10, f\"Line: {line}\\nScore: {score}\")\n",
        "            pdf.ln(2)\n",
        "\n",
        "        # Add Word Cloud\n",
        "        if attribute in wordclouds:\n",
        "            plt.imshow(wordclouds[attribute], interpolation='bilinear')\n",
        "            plt.axis(\"off\")\n",
        "            plt.savefig(f\"{attribute}_wordcloud.png\")\n",
        "            pdf.image(f\"{attribute}_wordcloud.png\", x=10, y=80, w=180)\n",
        "            plt.close()\n",
        "\n",
        "    pdf.output(output_file)\n",
        "    return output_file"
      ],
      "metadata": {
        "id": "TVCpFVhKIeBl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_keywords_and_video(url, excel_file):\n",
        "    metadata, error = fetch_video_metadata(url)\n",
        "    if error:\n",
        "        return error, None\n",
        "\n",
        "    transcript, error = fetch_transcript(url)\n",
        "    if error:\n",
        "        return error, None\n",
        "\n",
        "    sentences = split_long_sentences(transcript)\n",
        "    keywords, attributes = read_keywords(excel_file)\n",
        "    matched_keywords = match_keywords_in_sentences(sentences, keywords)\n",
        "    sentiment_results = analyze_sentiment_for_keywords(matched_keywords, sentences)\n",
        "    wordclouds = generate_word_clouds(matched_keywords)\n",
        "    pdf_file = generate_pdf_with_sections(metadata, sentiment_results, wordclouds)\n",
        "\n",
        "    return \"Processing completed successfully!\", pdf_file"
      ],
      "metadata": {
        "id": "_x2JeaTxIeEW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, VideoUnavailable\n",
        "from googleapiclient.discovery import build\n",
        "from fpdf import FPDF\n",
        "import pandas as pd\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.parse\n",
        "import gradio as gr\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.styles import Font, Alignment\n",
        "\n",
        "# Initialize Spacy and VADER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# YouTube Data API key\n",
        "YOUTUBE_API_KEY = \"XXXXXXXXXXXXXXXXXXXXXXXX\"\n",
        "\n",
        "def process_keywords_and_video(url, excel_file):\n",
        "    metadata, error = fetch_video_metadata(url)\n",
        "    if error:\n",
        "        return error, None\n",
        "\n",
        "    transcript, error = fetch_transcript(url)\n",
        "    if error:\n",
        "        return error, None\n",
        "\n",
        "    sentences = split_long_sentences(transcript)\n",
        "    keywords, attributes = read_keywords(excel_file)\n",
        "    matched_keywords = match_keywords_in_sentences(sentences, keywords)\n",
        "    sentiment_results = analyze_sentiment_for_keywords(matched_keywords, sentences)\n",
        "    wordclouds = generate_word_clouds(matched_keywords)\n",
        "    pdf_file = generate_pdf_with_sections(metadata, sentiment_results, wordclouds)\n",
        "    generate_excel(sentiment_results, attributes)\n",
        "\n",
        "    return \"Processing completed successfully!\", pdf_file\n",
        "\n",
        "\n",
        "# Gradio App\n",
        "with gr.Blocks(\n",
        "    css=\"\"\"\n",
        "    body {\n",
        "        background: linear-gradient(120deg, #fdfbfb, #f8f6f7);\n",
        "        font-family: 'Arial', sans-serif;\n",
        "        color: #444;\n",
        "    }\n",
        "    .gr-textbox textarea, .gr-file input {\n",
        "        border: 1px solid #ccc;\n",
        "        border-radius: 8px;\n",
        "        padding: 10px;\n",
        "        box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n",
        "        background: #fff;\n",
        "    }\n",
        "    .gr-textbox textarea:focus, .gr-file input:focus {\n",
        "        outline: none;\n",
        "        border-color: #ff9a9e;\n",
        "        box-shadow: 0 4px 10px rgba(255,154,158,0.5);\n",
        "    }\n",
        "    .btn-custom {\n",
        "        background-color: #ff6f61;\n",
        "        color: white;\n",
        "        padding: 12px 24px;\n",
        "        font-size: 16px;\n",
        "        font-weight: bold;\n",
        "        border: none;\n",
        "        border-radius: 8px;\n",
        "        cursor: pointer;\n",
        "    }\n",
        "    .btn-custom:hover {\n",
        "        background-color: #e65550;\n",
        "    }\n",
        "    .title-box {\n",
        "        background-color: #ff6f61;\n",
        "        color: white;\n",
        "        padding: 20px;\n",
        "        text-align: center;\n",
        "        font-size: 2.5em;\n",
        "        font-weight: bold;\n",
        "        border-radius: 8px;\n",
        "        box-shadow: 0 4px 10px rgba(0,0,0,0.2);\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "    \"\"\"\n",
        ") as iface:\n",
        "    gr.Markdown('<div class=\"title-box\">Auto-Insight: YouTube Video Analyzer for Automobiles</div>')\n",
        "    with gr.Row():\n",
        "        video_url = gr.Textbox(label=\"YouTube Video URL\", placeholder=\"Enter the YouTube video URL\")\n",
        "        excel_file = gr.File(label=\"Upload Excel File with Keywords\")\n",
        "    with gr.Row():\n",
        "        process_button = gr.Button(\"Analyze Video\", elem_classes=[\"btn-custom\"])\n",
        "    with gr.Row():\n",
        "        processing_status = gr.Textbox(label=\"Processing Status\", interactive=False)\n",
        "    with gr.Row():\n",
        "        pdf_output = gr.File(label=\"Download Sentiment Report (PDF)\")\n",
        "        excel_output = gr.File(label=\"Download Sentiment Report (Excel)\")\n",
        "\n",
        "    def process_with_excel(url, excel_file):\n",
        "        status, pdf_path = process_keywords_and_video(url, excel_file)\n",
        "        return status, pdf_path, \"sentiment_analysis_results.xlsx\"\n",
        "\n",
        "    process_button.click(\n",
        "        process_with_excel,\n",
        "        inputs=[video_url, excel_file],\n",
        "        outputs=[processing_status, pdf_output, excel_output]\n",
        "    )\n",
        "\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "A1eY37JaNlZD",
        "outputId": "2d957afd-0b35-43c9-a9b9-5143bc90f73c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f5e27c3414970f8326.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f5e27c3414970f8326.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, VideoUnavailable\n",
        "from googleapiclient.discovery import build\n",
        "from fpdf import FPDF\n",
        "import pandas as pd\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.parse\n",
        "import gradio as gr\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.styles import Font, Alignment\n",
        "\n",
        "# Initialize Spacy and VADER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# YouTube Data API key\n",
        "YOUTUBE_API_KEY = \"XXXXXXXXXXXXXX\"\n",
        "\n",
        "\n",
        "\n",
        "def process_keywords_and_video(url, excel_file):\n",
        "    # Indent the code block within the function definition\n",
        "    metadata, error = fetch_video_metadata(url)\n",
        "    if error:\n",
        "        return error, None\n",
        "\n",
        "    transcript, error = fetch_transcript(url)\n",
        "    if error:\n",
        "        return error, None\n",
        "\n",
        "    sentences = split_long_sentences(transcript)\n",
        "    keywords, attributes = read_keywords(excel_file)\n",
        "    matched_keywords = match_keywords_in_sentences(sentences, keywords)\n",
        "    sentiment_results = analyze_sentiment_for_keywords(matched_keywords, sentences)\n",
        "    wordclouds = generate_word_clouds(matched_keywords)\n",
        "    pdf_file = generate_pdf_with_sections(metadata, sentiment_results, wordclouds)\n",
        "    # Generate Excel file after processing the video to include results\n",
        "    generate_excel(sentiment_results, attributes) # Added this line to generate an excel file after processing is completed\n",
        "\n",
        "    return \"Processing completed successfully!\", pdf_file\n",
        "\n",
        "\n",
        "# Gradio App\n",
        "with gr.Blocks() as iface:\n",
        "    gr.Markdown(\"<h1>Auto-Insight: YouTube Video Analyzer for Automobiles</h1>\")\n",
        "    video_url = gr.Textbox(label=\"YouTube Video URL\", placeholder=\"Enter the YouTube video URL\")\n",
        "    excel_file = gr.File(label=\"Upload Excel File with Keywords\")\n",
        "    process_button = gr.Button(\"Analyze Video\")\n",
        "    processing_status = gr.Textbox(label=\"Processing Status\", interactive=False)\n",
        "    pdf_output = gr.File(label=\"Download Sentiment Report (PDF)\")\n",
        "    excel_output = gr.File(label=\"Download Sentiment Report (Excel)\") #Added excel output\n",
        "\n",
        "    def process_with_excel(url, excel_file):\n",
        "      status, pdf_path = process_keywords_and_video(url, excel_file)\n",
        "      return status, pdf_path, \"sentiment_analysis_results.xlsx\"  # Return the Excel file path\n",
        "\n",
        "    process_button.click(\n",
        "        process_with_excel,\n",
        "        inputs=[video_url, excel_file],\n",
        "        outputs=[processing_status, pdf_output, excel_output] #Modified the outputs\n",
        "    )\n",
        "\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "h5ubRypnIeGk",
        "outputId": "376a7f77-eb52-4570-f5a9-ba5141400f65"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://004de2daac533efcc0.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://004de2daac533efcc0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x_mHy6Hdo3_6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}